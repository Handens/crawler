# 爬虫

#### 1、爬虫概念

​	爬虫是什么？

​	互联网：网状节点就是a链接（url） 同意资源定位符

​	互联网爬虫：写一个程序、根据url去抓取指定的内容

​	都有哪些语言可以实现爬虫

​	1）php，号称世界上最好的语言（自称）（俗称：看黄片的语言）

​			后端语言：php，java，python，可以实现爬虫，做的不好，天生对多进程和多线程支持的不好

​	2）java，可以实现，而且做的非常好，是python爬虫最主要的竞争对手，做的不好，语言不简洁，代码臃肿，重构成本高。

​	3)c/c++，也可以实现爬虫，非常强大。

​	4）python，可以实现爬虫。号称世界上最优雅的语言，代码简洁，学习成本低，执行效率非常高。还有一个非常强大的爬虫框架   scrapy

通用爬虫：

​	例子：百度、谷歌、360、搜狗、必应等等，搜索引擎

​	做的工作：

​		1）爬去互联网所有的数据

​		2）对数据存储并且处理

​		3）给用户提供检索服务

​		如何让百度抓取你的网站

​		1）静静地等待

​		2）主动提交自己的url

​		3）在其他网站设置友情链接

​		我的网站不想让百度抓取

​			君子协议，口头协议，robots协议

​			存放到网站的根目录下，限制搜索引擎可以爬去哪些，不可以爬去哪些

​		网站排名（SEO）

​		（1）PageRank值排名，根据点击量、浏览量等

​		（2）竞价排名

##### 	



##### 	通用抓取的缺点：

​			1）抓取很多数据都是无效的

​			2）不能根据自己的需求抓取数据

##### 	

##### 	聚焦爬虫：

​	根据自己特定的需求，来抓取指定的数据

​	如何实现聚焦爬虫?

​	网页的特点：

​	1）网页都有自己唯一的url

​	2）网页都是有html组成

​	3）网页传输都是使用http、https协议



##### 	思路：

​		1）提供一个url

​		2）模拟浏览器发送http请求

​		3）从html结构中提取指定的数据，从字符串中根据规则提取指定数据

​	

##### 	开发环境：

​		window系统，python  3.6（64位），sublime编辑器，pycharm编辑器，vscode，



##### 	整体内容：

​	1）设计到python的库

​		urllib，requests，selenium，jsonpath，lxml等一些库

​	2）解析内容

​		正则表达式，bs4解析，xpath解析，jsonpath解析

​	3）采集动态html

​		DOM操作，动态的添加或者删除节点，selenium+pathtomjs

​	4）scrapy框架

​		异步高性能网络爬虫框架的学习

​	5）scrapy-redis分布式部署

​		在scrapy的基础上，多了一套分布式部署的组件

​	6）设计到的爬虫-反爬虫-反反爬虫之间的斗争

​		反爬会伤害真实的用户，一般情况下，反爬也就这么两点：第一个UA，第二个封ip，第三个验证码，光学识别 ，打码平台

​		换思路解决问题：其他网站，手机端等

#### 2、http协议

​	什么是协议？ 协议就是规定好的传输方式

​	上网原理：![看图片](C:\Users\Administrator\Desktop\QQ截图20180807141411.png)



​	锚点：跳转可以实现本页面的跳转

​	一个网页要呈现在你面前，一般需要15-30个请求

​	不单单是html ，js，css等都是要单独发送请求的



##### 	http和https的区别：

​	1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。

　　2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。

　　3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。

　　4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。

​	

​	加密：公钥私钥

​	客户端：123456用秘钥加密=====》服务端：用秘钥解密得到123456

​	秘钥相同称之为对称加解密

​	秘钥不相同称之为非对称加解密

​	客户端：123456用公钥解密====》服务端：用私钥解密得到123456

​	http协议学习

​	图解协议

​	http协议是基于tcp的，交互之前建立连接，三次握手

​	udp传输协议，只需要知道ip和端口饥渴发送消息

​	

​	请求：包含请求行、请求头、请求内容

​		请求行：请求方式、请求资源、协议版本号

​		请求头：

​			accept:浏览器通过这个头告诉服务器，它所支持的数据类型
　　		Accept-Charset: 浏览器通过这个头告诉服务器，它支持哪种字符集
　　		Accept-Encoding：浏览器通过这个头告诉服务器，支持的压缩格式
　　		Accept-Language：浏览器通过这个头告诉服务器，它的语言环境
　　		Host：浏览器通过这个头告诉服务器，想访问哪台主机
　　		If-Modified-Since: 浏览器通过这个头告诉服务器，缓存数据的时间
　　		Referer：浏览器通过这个头告诉服务器，客户机是哪个页面来的  防盗链
　　		Connection：浏览器通过这个头告诉服务器，请求完后是断开链接还是何持链接

​			X-Requested-With:XMLHttpRequest    代表ajax请求

​	http响应：响应行、响应头、响应内容

响应行里面，常见的状态码
			响应头：对我们来说不重要
				Location: 服务器通过这个头，来告诉浏览器跳到哪里
			　　Server：服务器通过这个头，告诉浏览器服务器的型号
			　　Content-Encoding：服务器通过这个头，告诉浏览器，数据的压缩格式
			　　Content-Length: 服务器通过这个头，告诉浏览器回送数据的长度
			　　Content-Language: 服务器通过这个头，告诉浏览器语言环境
			　　Content-Type：服务器通过这个头，告诉浏览器回送数据的类型
			　　Refresh：服务器通过这个头，告诉浏览器定时刷新
			　　Content-Disposition: 服务器通过这个头，告诉浏览器以下载方式打数据
			　　Transfer-Encoding：服务器通过这个头，告诉浏览器数据是以分块方式回送的
			　　Expires: -1  控制浏览器不要缓存
			　　Cache-Control: no-cache  
			　　Pragma: no-cache
			响应内容：html、css、js、图片

```
https://www.cnblogs.com/wqhwe/p/5407468.html
https://www.cnblogs.com/10158wsj/p/6762848.html
```



#### 3、抓包工具



​	（1）谷歌浏览器自带抓包工具
		右键开发者工具==》network
		XHR: XMLHttpReqeust  前端要想发送ajax请求，通过它创建对象，发送请求
		query_string_parameters : 请求字符串，get参数
		formdata ： 如果是post参数
	（2）专业工具，fiddler
		专业抓包工具，比谷歌强在了跳转的时候很多请求都能抓取到
		见文档



#### 4、urllib库

​	urllib库是什么？自带的python库，模拟发送http请求
	python 2系列：urllib  urllib2
	python 3系列：urllib  
		urllib.request  模拟发送请求
			urlopen(url) : 向url发送请求，得到响应对象
			urlretrieve(url, filepath) ： 向url发送请求，直接将响应写入到filepath中
		response属性和方法
			字符串格式==》字节格式
				encode('utf8')
			字节格式==》字符串格式
				decode('gbk')
			response.read()  : 读取字节格式内容
			response.url : 获取请求url
			response.headers: 响应头部
			response.status: 状态码
		

		下载图片：得到图片的src属性，就可以将图片下载到本地
		urllib.parse    处理参数或者url
		urllib.error    如何处理异常
	sublime使用方法：
		插件安装：首先安装插件管理器（package control）
			见文档
		安装插件：ctrl+shift+p
			imesupport  解决光标跟随问题
			anaconda    python智能提示工具
			emmet       html快速插件
		【注】sublime安装完毕之后，默认会自动关联python的可执行程序，如果没有自动关联，点击tools里面的build system，手动选择python，如果还不行，自己按照固定的格式新建一个build system即可，如果还不行，算了，别用了
